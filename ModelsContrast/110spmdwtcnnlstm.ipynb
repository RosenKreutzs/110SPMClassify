{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3843f2f",
   "metadata": {
    "papermill": {
     "duration": 0.005674,
     "end_time": "2024-08-06T14:35:58.459137",
     "exception": false,
     "start_time": "2024-08-06T14:35:58.453463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# step1:处理数据"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f1d762c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:35:58.471375Z",
     "iopub.status.busy": "2024-08-06T14:35:58.470949Z",
     "iopub.status.idle": "2024-08-06T14:36:10.879020Z",
     "shell.execute_reply": "2024-08-06T14:36:10.877784Z"
    },
    "papermill": {
     "duration": 12.417142,
     "end_time": "2024-08-06T14:36:10.881692",
     "exception": false,
     "start_time": "2024-08-06T14:35:58.464550",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-08T09:32:27.147126Z",
     "start_time": "2024-08-08T09:32:21.978160Z"
    }
   },
   "source": [
    "import os  \n",
    "import torch  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "    #step1:加载数据集\n",
    "# 设置数据集的路径  \n",
    "data_dir = '../DataFiles/ODS/strain'   \n",
    "# 创建空的列表来存储所有的DataFrame  \n",
    "X_data0 = []  \n",
    "y_data0 = []\n",
    "# 遍历指定目录下的所有文件  \n",
    "for filename in os.listdir(data_dir):  \n",
    "    if filename.endswith('.csv'):  \n",
    "        # 构建文件的完整路径  \n",
    "        file_path = os.path.join(data_dir, filename)  \n",
    "        # 读取Excel文件  \n",
    "        df = pd.read_csv(file_path)\n",
    "        serie=df.iloc[:, 0]\n",
    "        version_without_extension = filename.rsplit(\".\", 1)[0] \n",
    "        serie.name=version_without_extension\n",
    "        # 将读取的DataFrame添加到列表中  \n",
    "        X_data0.append(serie)\n",
    "        y_data0.append(version_without_extension)\n",
    "\n",
    "    #step2:遍历X_data0中的每个DataFrame，并删除非数字数据  \n",
    "cleaned_data = []  \n",
    "for df in X_data0:  \n",
    "    df_cleaned = df[pd.to_numeric(df, errors='coerce').notnull()]  \n",
    "    cleaned_data.append(df_cleaned)  \n",
    " \n",
    "\n",
    "    #step3:将X_data0中的每个DataFrame的所有元素变为float类型 \n",
    "for i, df in enumerate(cleaned_data):  \n",
    "    X_data0[i] = df.astype(float) \n",
    "\n",
    "# 初始化X_data1来存储结果  \n",
    "X_data1 = []  \n",
    "  \n",
    "# 遍历X_data0中的每个Series  \n",
    "for series in X_data0:  \n",
    "    # 计算步长（由于重叠率为50%，步长为序列长度的一半）  \n",
    "    step_size = 1024  # 因为2048的一半是1024，且我们想要50%的重叠  \n",
    "    # 初始化一个空的list来存储子序列  \n",
    "    subsequences = []  \n",
    "    # 使用range和步长来生成子序列  \n",
    "    for i in range(0, len(series) - 2048+ 1, step_size):  \n",
    "        # 确保切片是一维的，并转换为Series（虽然这里切片已经是Series）  \n",
    "        subsequence = series.iloc[i:i+2048]  \n",
    "        subsequences.append(subsequence) \n",
    "    X_data1.append(subsequences)  \n",
    "type(X_data1),len(X_data1),len(X_data1[0])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2, 1814)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3b2845e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:36:10.894268Z",
     "iopub.status.busy": "2024-08-06T14:36:10.893880Z",
     "iopub.status.idle": "2024-08-06T14:36:26.331157Z",
     "shell.execute_reply": "2024-08-06T14:36:26.329660Z"
    },
    "papermill": {
     "duration": 15.44715,
     "end_time": "2024-08-06T14:36:26.334179",
     "exception": false,
     "start_time": "2024-08-06T14:36:10.887029",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:23.042851Z",
     "start_time": "2024-08-08T09:32:27.163234Z"
    }
   },
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from keras.utils import to_categorical\n",
    "import time  \n",
    "from keras.callbacks import Callback  \n",
    "import torch \n",
    "\n",
    "# 假设 X_data1 已经定义好了  \n",
    "all_data = []  \n",
    "all_labels = []  \n",
    "  \n",
    "for sublist in X_data1:  \n",
    "    for series in sublist:  \n",
    "        # 假设每个series是pandas Series  \n",
    "        all_data.append(series.values)  \n",
    "        all_labels.append(series.name)  \n",
    "  \n",
    "# 将数据转换为numpy数组  \n",
    "data_array = np.array(all_data)  \n",
    "# 将标签转换为编码  \n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "labels_encoded = label_encoder.fit_transform(all_labels)  \n",
    "  \n",
    "# 如果需要one-hot编码  \n",
    "labels_one_hot = to_categorical(labels_encoded)  \n",
    "  \n",
    "# 这里你可能需要reshape data_array 以符合LSTM的输入要求  \n",
    "# LSTM通常要求输入形状为 (samples, time_steps, features)  \n",
    "# 假设每个series的长度相同，或者你可以填充/截断它们以具有相同的长度  \n",
    "# time_steps = data_array.shape[1]  # 假设所有series长度相同  \n",
    "# data_array = data_array.reshape(-1, time_steps, 1)  # 增加一个维度以匹配LSTM的输入要求\n",
    "\n",
    "# 分割数据集  \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_array, labels_one_hot, test_size=0.3, random_state=42)  \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)  \n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)  \n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)  \n",
    "y_val = torch.tensor(y_val, dtype=torch.float32) \n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)  \n",
    "y_test = torch.tensor(y_test, dtype=torch.float32) \n",
    "type(X_train),X_train.shape,type(y_train),y_train.shape\n",
    "\n",
    "# 如果您的数据不适合内存中的批处理，您可能需要使用 DataLoader  \n",
    "# 但为了简单起见，这里我们手动进行批处理  \n",
    "from torch.utils.data import TensorDataset, DataLoader  \n",
    "batch_size=1\n",
    "traindataset = TensorDataset(X_train, y_train)  \n",
    "trainloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True)  \n",
    "\n",
    "valdataset = TensorDataset(X_val, y_val)  \n",
    "valloader = DataLoader(valdataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "testdataset = TensorDataset(X_test, y_test)  \n",
    "testloader = DataLoader(testdataset, batch_size=batch_size, shuffle=True) "
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "4d236bee",
   "metadata": {
    "papermill": {
     "duration": 0.006834,
     "end_time": "2024-08-06T14:36:26.354910",
     "exception": false,
     "start_time": "2024-08-06T14:36:26.348076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# step2:构建小波卷积层(用的是离散小波变换)\n",
    "- 利用离散小波函数的高通滤波器(一维窗口)和低通滤波器(一维窗口)各自作为卷积核，获得两个原始信号在各自滤波器的投影一维序列；\n",
    "- 高通滤波器的投影一维序列包含了高频信号部分，多是以细节和噪音信息为主；\n",
    "- 低通滤波器的投影一维序列包含了低频信号部分，多是以大致趋势为主；\n",
    "- 待估参数是两个滤波器中的数值；"
   ]
  },
  {
   "cell_type": "code",
   "id": "67fe17c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:36:26.369280Z",
     "iopub.status.busy": "2024-08-06T14:36:26.368546Z",
     "iopub.status.idle": "2024-08-06T14:36:26.480539Z",
     "shell.execute_reply": "2024-08-06T14:36:26.479300Z"
    },
    "papermill": {
     "duration": 0.122733,
     "end_time": "2024-08-06T14:36:26.483427",
     "exception": false,
     "start_time": "2024-08-06T14:36:26.360694",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:23.410410Z",
     "start_time": "2024-08-08T09:33:23.075467Z"
    }
   },
   "source": [
    "import pywt\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "def create_wavelet_filter(wave, in_size, out_size, type=torch.float):\n",
    "    # 创建一维小波对象\n",
    "    w = pywt.Wavelet(wave)\n",
    "    # 生成降采样滤波器\n",
    "    dec_hi = torch.tensor(w.dec_hi[::-1], dtype=type)\n",
    "    dec_lo = torch.tensor(w.dec_lo[::-1], dtype=type)\n",
    "    dec_filters = torch.stack([dec_lo.unsqueeze(0),\n",
    "                               dec_hi.unsqueeze(0)], dim=0)\n",
    "    dec_filters = dec_filters.repeat(in_size,1,1)\n",
    "    # 生成重建滤波器\n",
    "    rec_hi = torch.tensor(w.rec_hi[::-1], dtype=type).flip(dims=[0])\n",
    "    rec_lo = torch.tensor(w.rec_lo[::-1], dtype=type).flip(dims=[0])\n",
    "    rec_filters = torch.stack([rec_lo.unsqueeze(0),\n",
    "                               rec_hi.unsqueeze(0)], dim=0)\n",
    "    rec_filters = rec_filters.repeat(out_size, 1, 1)\n",
    "    return dec_filters, rec_filters\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def wavelet_transform(x, filters):\n",
    "    # x 的形状应为 (b, c, w) 其中 w 是宽度（或序列长度）\n",
    "    b, c, w = x.shape\n",
    "    x = F.conv1d(x, filters, stride=2)\n",
    "    x = x.reshape(b, c, 2, w // 2)\n",
    "    return x\n",
    "\n",
    "def inverse_wavelet_transform(x, filters):\n",
    "    # x 的形状应为 (b, c * 4, w_half)\n",
    "    b, c, _, w_half = x.shape\n",
    "    pad = (filters.shape[2] // 2 - 1,)\n",
    "    x = x.reshape(b, c * 2, w_half)\n",
    "    x = F.conv_transpose1d(x, filters, stride=2, groups=c, padding=pad)\n",
    "    return x\n",
    "class _ScaleModule(nn.Module):\n",
    "    def __init__(self, dims, init_scale=1.0, init_bias=0):\n",
    "        super(_ScaleModule, self).__init__()\n",
    "        self.dims = dims\n",
    "        self.weight = nn.Parameter(torch.ones(*dims) * init_scale)\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mul(self.weight, x)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "class WTConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, bias=True, wt_levels=1, wt_type='db1'):\n",
    "        super(WTConv1d, self).__init__()\n",
    "        assert in_channels == out_channels\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.wt_levels = wt_levels\n",
    "        self.stride = stride\n",
    "        self.dilation = 1#即卷积核元素之间没有间距\n",
    "\n",
    "        # 生成的滤波器\n",
    "        self.wt_filter, self.iwt_filter = create_wavelet_filter(wt_type, in_channels, in_channels, torch.float)\n",
    "        #nn.Parameter 是一个特殊的 Tensor，它被注册为模型的一个参数，并会在训练过程中自动更新。\n",
    "        #requires_grad=False 表示这个参数在训练过程中不需要计算梯度，因此不会被优化算法更新。\n",
    "        self.wt_filter = nn.Parameter(self.wt_filter, requires_grad=False)\n",
    "        self.iwt_filter = nn.Parameter(self.iwt_filter, requires_grad=False)\n",
    "        # functools.partial 函数，它的作用是创建一个新的函数，该函数已经预先填充了一些参数\n",
    "        self.wt_function = partial(wavelet_transform, filters=self.wt_filter)\n",
    "        self.iwt_function = partial(inverse_wavelet_transform, filters=self.iwt_filter)\n",
    "\n",
    "        # 基础卷积和缩放模块\n",
    "        self.base_conv = nn.Conv1d(in_channels, in_channels, kernel_size, padding='same', stride=1, dilation=1, groups=in_channels, bias=bias)\n",
    "        self.base_scale = _ScaleModule([1, in_channels, 1])\n",
    "\n",
    "        # 小波卷积层和缩放\n",
    "        self.wavelet_convs = nn.ModuleList(\n",
    "            [nn.Conv1d(in_channels * 2, in_channels * 2, kernel_size, padding='same', stride=1, dilation=1, groups=in_channels * 2, bias=False) for _ in range(self.wt_levels)]\n",
    "        )\n",
    "        self.wavelet_scale = nn.ModuleList(\n",
    "            [_ScaleModule([1, in_channels * 2, 1], init_scale=0.1) for _ in range(self.wt_levels)]\n",
    "        )\n",
    "\n",
    "        if self.stride > 1:\n",
    "            self.stride_filter = nn.Parameter(torch.ones(in_channels, 1, 1), requires_grad=False)\n",
    "            self.do_stride = lambda x_in: F.conv1d(x_in, self.stride_filter, bias=None, stride=self.stride, groups=in_channels)\n",
    "        else:\n",
    "            self.do_stride = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ll_in_levels = []\n",
    "        x_h_in_levels = []\n",
    "        shapes_in_levels = []\n",
    "\n",
    "        curr_x_ll = x.unsqueeze(1)\n",
    "        for i in range(self.wt_levels):\n",
    "            curr_shape = curr_x_ll.shape\n",
    "            shapes_in_levels.append(curr_shape)\n",
    "            #如果当前层级的宽度不是偶数，使用 F.pad 函数对 curr_x_ll 进行填充，以保证宽度是偶数。\n",
    "            if curr_shape[2] % 2 > 0:\n",
    "                curr_pads = (curr_shape[2] % 2,)\n",
    "                curr_x_ll = F.pad(curr_x_ll, curr_pads)\n",
    "            curr_x = self.wt_function(curr_x_ll)\n",
    "            #从 curr_x 中提取低频部分（第 0 个索引），更新 curr_x_ll。\n",
    "            #将 curr_x 重塑为 (b, c * 2, w // 2)。\n",
    "            #通过 self.wavelet_convs[i] 和 self.wavelet_scale[i] 对 curr_x_tag 进行卷积和缩放操作。\n",
    "            curr_x_ll = curr_x[:, :, 0, :]\n",
    "            curr_x_h = curr_x[:, :, 1, :]\n",
    "            x_ll_in_levels.append(curr_x_ll)\n",
    "            x_h_in_levels.append(curr_x_h)\n",
    "        \n",
    "        if self.stride:\n",
    "            if self.do_stride is not None:\n",
    "                x_ll_in_levels = [self.do_stride(x) for x in x_ll_in_levels]\n",
    "                x_h_in_levels = [self.do_stride(x) for x in x_h_in_levels]\n",
    "        return x_ll_in_levels[0],x_h_in_levels[0]\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "f0ab1a21",
   "metadata": {
    "papermill": {
     "duration": 0.005216,
     "end_time": "2024-08-06T14:36:26.494196",
     "exception": false,
     "start_time": "2024-08-06T14:36:26.488980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# step3:构建完整model结构"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b3731ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:36:26.507105Z",
     "iopub.status.busy": "2024-08-06T14:36:26.506422Z",
     "iopub.status.idle": "2024-08-06T14:36:27.963579Z",
     "shell.execute_reply": "2024-08-06T14:36:27.962337Z"
    },
    "papermill": {
     "duration": 1.466491,
     "end_time": "2024-08-06T14:36:27.966062",
     "exception": false,
     "start_time": "2024-08-06T14:36:26.499571",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-08T09:36:16.749445Z",
     "start_time": "2024-08-08T09:36:16.654796Z"
    }
   },
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "  \n",
    "class ConvBiLSTMModel(nn.Module):  \n",
    "    def __init__(self, input_channels=1, hidden_size=64, num_classes=12):  \n",
    "        super(ConvBiLSTMModel, self).__init__()  \n",
    "        # 第一层小波卷积  \n",
    "        self.wtconv = WTConv1d(in_channels=1, out_channels=1)   \n",
    "        # BiLSTM层  \n",
    "        self.bilstm_ll = nn.LSTM(input_size=1, hidden_size=hidden_size,  \n",
    "                              num_layers=1, bidirectional=True,  \n",
    "                              batch_first=True)  \n",
    "        self.bilstm_h = nn.LSTM(input_size=1, hidden_size=hidden_size,  \n",
    "                              num_layers=1, bidirectional=True,  \n",
    "                              batch_first=True)  \n",
    "        # 线性层  \n",
    "        self.fc = nn.Linear(hidden_size * 4, num_classes)  # *2 因为是双向的  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        # 通过卷积层  \n",
    "        x_ll,x_h =self.wtconv(x) \n",
    "        # 调整维度以匹配LSTM的输入要求 [batch_size, seq_len, num_features]  \n",
    "        # LSTM需要 (seq_len, batch, input_size) 但我们设置batch_first=True \n",
    "        # LSTM的输出是 (output, (h_n, c_n))  \n",
    "        # output: [batch_size, seq_len, num_directions * hidden_size]  \n",
    "        # 我们只关心最后一个时间步的输出  \n",
    "        # 使用permute来交换维度  \n",
    "        x_ll = x_ll.permute(0, 2, 1)  \n",
    "        x_h = x_h.permute(0, 2, 1)\n",
    "        lstm_out_ll, _ = self.bilstm_ll(x_ll)  \n",
    "        lstm_out_ll = lstm_out_ll[:, -1, :]  # 取最后一个时间步的输出  \n",
    "        lstm_out_h, _ = self.bilstm_h(x_h)  \n",
    "        lstm_out_h = lstm_out_h[:, -1, :]  # 取最后一个时间步的输出  \n",
    "        lstm_out=torch.cat((lstm_out_ll, lstm_out_h), dim=1)\n",
    "        # 通过全连接层  \n",
    "        x = self.fc(lstm_out)  \n",
    "          \n",
    "        return x  \n",
    "\n",
    "# 创建模型实例  \n",
    "model = ConvBiLSTMModel()  \n",
    "  \n",
    "# 如果需要，可以添加损失函数和优化器  \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "3a8ac6ae",
   "metadata": {
    "papermill": {
     "duration": 0.005297,
     "end_time": "2024-08-06T14:36:27.977010",
     "exception": false,
     "start_time": "2024-08-06T14:36:27.971713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# step4:训练模型"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a9b8d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T14:36:27.990466Z",
     "iopub.status.busy": "2024-08-06T14:36:27.989594Z",
     "iopub.status.idle": "2024-08-06T18:41:31.341382Z",
     "shell.execute_reply": "2024-08-06T18:41:31.340396Z"
    },
    "papermill": {
     "duration": 14721.486983,
     "end_time": "2024-08-06T18:41:49.469548",
     "exception": false,
     "start_time": "2024-08-06T14:36:27.982565",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-08T09:36:31.374190Z",
     "start_time": "2024-08-08T09:36:21.889014Z"
    }
   },
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import time   \n",
    "from tqdm import tqdm  # 导入tqdm库\n",
    "import pickle  \n",
    "\n",
    "# 训练模型  \n",
    "num_epochs = 100  # 训练轮次  \n",
    "start_time = time.time()  \n",
    "elapsed_time = 0  \n",
    "  \n",
    "#训练模型  \n",
    "list_train_loss=[]\n",
    "list_train_accuracy=[]\n",
    "list_val_loss=[]\n",
    "list_val_accuracy=[]\n",
    "# 训练循环  \n",
    "for epoch in range(num_epochs):  \n",
    "    model.train()  # 设置模型为训练模式  \n",
    "    train_running_loss = 0.0  # 用于记录当前epoch的训练损失  \n",
    "    train_correct = 0  # 用于计算准确率的变量  \n",
    "    train_total = 0  # 总样本数  \n",
    "    # 使用tqdm包装训练数据加载器以显示进度条  \n",
    "    with tqdm(trainloader, desc=f'Train Epoch {epoch+1}/{num_epochs}', unit='batch') as t:  \n",
    "        for batch_idx, (inputs, labels) in enumerate(t):  \n",
    "            # 前向传播  \n",
    "            outputs = model(inputs)  \n",
    "            loss = criterion(outputs, labels)  \n",
    "            # 计算准确率  \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels, 1)  \n",
    "            train_total += labels.size(0)  \n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            # 反向传播和优化  \n",
    "            optimizer.zero_grad()  # 清除之前的梯度  \n",
    "            loss.backward()        # 反向传播计算梯度  \n",
    "            optimizer.step()       # 更新参数  \n",
    "            \n",
    "            train_running_loss += loss.item()  # 累加训练损失  \n",
    "            # 更新进度条信息  \n",
    "            t.set_postfix(loss=loss.item(), accuracy=100. * train_correct / train_total,train_correct=train_correct,train_total=train_total)  \n",
    "  \n",
    "    # 计算平均训练损失和准确率\n",
    "    train_loss = train_running_loss / len(trainloader.dataset) \n",
    "    train_accuracy = 100. * train_correct / train_total  \n",
    "    list_train_loss.append(train_loss)\n",
    "    list_train_accuracy.append(train_accuracy)\n",
    "    # 验证模型  \n",
    "    model.eval()  # 设置模型为评估模式  \n",
    "    val_running_loss = 0.0  # 用于记录当前epoch的训练损失  \n",
    "    val_correct = 0  # 用于计算准确率的变量  \n",
    "    val_total = 0  # 总样本数  \n",
    "    # 使用tqdm包装训练数据加载器以显示进度条  \n",
    "    with tqdm(valloader, desc=f'Val Epoch {epoch+1}/{num_epochs}', unit='batch') as v:\n",
    "        with torch.no_grad():  # 不需要计算梯度  \n",
    "            for batch_idx, (inputs, labels) in enumerate(v):  \n",
    "                # 前向传播  \n",
    "                outputs = model(inputs)  \n",
    "                loss = criterion(outputs, labels)  \n",
    "                # 计算准确率  \n",
    "                _, predicted = torch.max(outputs.data, 1)  \n",
    "                _, labels = torch.max(labels, 1)  \n",
    "                val_total += labels.size(0)  \n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_running_loss += loss.item()  # 累加训练损失  \n",
    "                # 更新进度条信息  \n",
    "                v.set_postfix(loss=loss.item(), accuracy=100. * val_correct / val_total)  \n",
    "  \n",
    "    # 计算平均训练损失和准确率\n",
    "    val_loss = val_running_loss / len(valloader.dataset)  \n",
    "    val_accuracy = 100. * val_correct / val_total  \n",
    "    list_val_loss.append(val_loss)\n",
    "    list_val_accuracy.append(val_accuracy)\n",
    "    torch.save(model.state_dict(), 'DWTCNNLSTM'+str(epoch)+'.pth')\n",
    "    # 显示验证损失和所用时间  \n",
    "    end_time = time.time()  \n",
    "    elapsed_time = end_time - start_time  \n",
    "    # 检查是否超过了5小时  \n",
    "    if elapsed_time > 4 * 60 * 60:  # 5 hours in seconds \n",
    "        print('Training time exceeded 5 hours. Saving model and exiting.')  \n",
    "        torch.save(model.state_dict(), 'DWTCNNLSTM.pth')   \n",
    "        break  \n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'DWTCNNLSTM.pth')\n",
    "print('Training complete.')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/100:   0%|          | 0/2412 [00:02<?, ?batch/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "214b3e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T18:42:56.282016Z",
     "iopub.status.busy": "2024-08-06T18:42:56.281556Z",
     "iopub.status.idle": "2024-08-06T18:42:56.965852Z",
     "shell.execute_reply": "2024-08-06T18:42:56.964683Z"
    },
    "papermill": {
     "duration": 33.766213,
     "end_time": "2024-08-06T18:42:56.969107",
     "exception": false,
     "start_time": "2024-08-06T18:42:23.202894",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:33.781555Z",
     "start_time": "2024-08-08T09:33:33.781555Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# 绘制训练loss和验证loss  \n",
    "plt.plot(list_train_loss, label='train')  \n",
    "plt.plot(list_val_loss, label='val')  \n",
    "plt.legend(loc='upper right')  \n",
    "plt.title('Model loss')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.show()  \n",
    "  \n",
    "# 绘制训练accuracy和验证accuracy  \n",
    "plt.plot(list_train_accuracy, label='train')  \n",
    "plt.plot(list_val_accuracy, label='val')  \n",
    "plt.legend(loc='lower right')  \n",
    "plt.title('Model accuracy')  \n",
    "plt.ylabel('Accuracy')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.ylim(ymin=0, ymax=100)  # 设置y轴的范围为0到1  \n",
    "plt.show()\n",
    "len(list_train_accuracy),len(list_val_accuracy)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a7d5eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T18:44:04.009380Z",
     "iopub.status.busy": "2024-08-06T18:44:04.008941Z",
     "iopub.status.idle": "2024-08-06T18:45:21.894502Z",
     "shell.execute_reply": "2024-08-06T18:45:21.892946Z"
    },
    "papermill": {
     "duration": 111.627232,
     "end_time": "2024-08-06T18:45:21.896952",
     "exception": false,
     "start_time": "2024-08-06T18:43:30.269720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from tqdm import tqdm  # 导入tqdm库\n",
    "model.eval()  # 设置模型为评估模式  \n",
    "test_running_loss = 0.0  # 用于记录当前epoch的训练损失  \n",
    "test_correct = 0  # 用于计算准确率的变量  \n",
    "test_total = 0  # 总样本数  \n",
    "with tqdm(testloader, desc=f'Test Epoch:', unit='batch') as t:\n",
    "        with torch.no_grad():  # 不需要计算梯度  \n",
    "            for batch_idx, (inputs, labels) in enumerate(t):  \n",
    "                # 前向传播  \n",
    "                outputs = model(inputs)  \n",
    "                loss = criterion(outputs, labels)  \n",
    "                # 计算准确率  \n",
    "                _, predicted = torch.max(outputs.data, 1)  \n",
    "                _, labels = torch.max(labels, 1)  \n",
    "                test_total += labels.size(0)  \n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                test_running_loss += loss.item()  # 累加训练损失  \n",
    "                # 更新进度条信息  \n",
    "                t.set_postfix(loss=loss.item(), accuracy=100. * test_correct / test_total)  \n",
    "  \n",
    "    # 计算平均训练损失和准确率\n",
    "test_loss = test_running_loss / len(testloader.dataset)  \n",
    "test_accuracy = 100. * test_correct / test_total \n",
    "\n",
    "test_loss,test_accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "print(help(pickle))"
   ],
   "id": "b35bbabd9e9174f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4457c158bf6075b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5319136,
     "sourceId": 8838704,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 97076,
     "modelInstanceId": 72145,
     "sourceId": 90121,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15003.789543,
   "end_time": "2024-08-06T18:45:58.685368",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-06T14:35:54.895825",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
